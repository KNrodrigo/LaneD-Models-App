{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = r\"..\\VIL100\"\n",
    "train_val_pth = f'{ROOT_DIR}\\\\data\\\\train.txt'\n",
    "test_pth = f'{ROOT_DIR}\\\\data\\\\test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read training and validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_img_pths = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(train_val_pth, 'r') as file:\n",
    "    # Read the entire contents of the file into a variable\n",
    "    file_contents = file.read()\n",
    "    # train_pths.append(file_contents)\n",
    "train_val_img_pths = file_contents.split(\"\\n\")\n",
    "\n",
    "train_val_img_pths = [pth.strip() for pth in train_val_img_pths]\n",
    "\n",
    "# Remove the last item (\"Empty string\")\n",
    "train_val_img_pths = train_val_img_pths[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full path to each training/validation images\n",
    "train_val_img_pths = [f'{ROOT_DIR}{pth}' for pth in train_val_img_pths]\n",
    "train_val_mask_pths = [pth.replace(\"JPEGImages\", \"Annotations\").replace(\"jpg\", \"png\") for pth in train_val_img_pths]\n",
    "# mask_paths = [f'{ROOT_DIR}/Annotations/{pth.replace(\"/JPEGImages/\", \"\").replace(\"jpg\", \"png\")}' for pth in img_pths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open an image file\n",
    "image = Image.open(train_val_img_pths[0]) \n",
    "\n",
    "# Get the size of the image (width, height)\n",
    "width, height = image.size\n",
    "\n",
    "print(f\"Image size: {width} x {height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the lists to map the training/validation original images to their annotations\n",
    "combined = list(zip(train_val_img_pths, train_val_mask_pths))\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Step 3: Unzip the shuffled list back into two lists\n",
    "train_img_pths, mask_pths = zip(*combined)  # Unpack the tuples back into separate lists\n",
    "\n",
    "# Convert the tuples back to lists (if needed)\n",
    "train_val_img_pths = list(train_val_img_pths)\n",
    "train_val_mask_pths = list(train_val_mask_pths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into a training and validation set\n",
    "VALIDATION_SPLIT = 0.2  # Percentage of data to use for validation\n",
    "\n",
    "# Split file paths and labels into training and validation sets\n",
    "split_index = int(len(train_val_img_pths) * VALIDATION_SPLIT)\n",
    "val_img_pths = train_val_img_pths[:split_index]\n",
    "val_mask_pths = train_val_mask_pths[:split_index]\n",
    "train_img_pths = train_val_img_pths[split_index:]\n",
    "train_mask_pths = train_val_mask_pths[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize images and add a batch dimension\n",
    "def resize_image(image, target_size=(960, 528)):\n",
    "    resized_image = tf.image.resize(image, target_size)\n",
    "    resized_image = tf.expand_dims(resized_image, axis=0)  # Add batch dimension\n",
    "    return resized_image\n",
    "\n",
    "# Function to resize masks and add a batch dimension\n",
    "def resize_mask(mask, target_size=(960, 528)):\n",
    "    resized_mask = tf.image.resize(mask, target_size)\n",
    "    resized_mask = tf.expand_dims(resized_mask, axis=0)  # Add batch dimension\n",
    "    return resized_mask\n",
    "\n",
    "# Function to load, normalize, and resize images\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decode RGB image\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)  # Normalize to [0, 1]\n",
    "    return image\n",
    "\n",
    "# Function to load, convert, and resize masks\n",
    "def load_mask(mask_path):\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.image.decode_png(mask, channels=1)  # Decode grayscale mask\n",
    "    mask = tf.where(mask > 0, 1.0, 0.0)  # Convert to binary masks if needed\n",
    "    return mask\n",
    "\n",
    "# Define a function to load, normalize, and resize images and masks\n",
    "def load_image_and_mask(image_path, mask_path):\n",
    "    image = load_image(image_path)\n",
    "    mask = load_mask(mask_path)\n",
    "\n",
    "    # Resize images and masks to (960, 528), ensuring batch dimension\n",
    "    image = resize_image(image)\n",
    "    mask = resize_mask(mask)\n",
    "    \n",
    "    return image, mask\n",
    "    # return tf.squeeze(image), tf.squeeze(mask)  # Squeeze to remove extra batch dimension\n",
    "\n",
    "# Create a TensorFlow training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_img_pths, train_mask_pths))\n",
    "train_dataset = train_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# Create a TensorFlow validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_img_pths, val_mask_pths))\n",
    "val_dataset = val_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a backbone from a list of backbones here: https://github.com/qubvel/segmentation_models\n",
    "BACKBONE = 'resnet101'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "# define model\n",
    "model = sm.PSPNet(BACKBONE, input_shape=(960, 528, 3), encoder_weights='imagenet', classes=1, activation='sigmoid')\n",
    "model.compile(\n",
    "    'Adam',\n",
    "    loss=sm.losses.dice_loss,\n",
    "    metrics=[sm.metrics.f1_score],\n",
    ")\n",
    "\n",
    "# fit model\n",
    "history = model.fit(\n",
    "   train_dataset,\n",
    "   batch_size=100,\n",
    "   epochs=50,\n",
    "   validation_data=val_dataset,\n",
    "   callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('model.keras')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/guide/keras/training_with_built_in_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_pths = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(test_pth, 'r') as file:\n",
    "    # Read the entire contents of the file into a variable\n",
    "    file_contents = file.read()\n",
    "    # train_pths.append(file_contents)\n",
    "test_img_pths = file_contents.split(\"\\n\")\n",
    "\n",
    "test_img_pths = test_img_pths[:-1]\n",
    "test_img_pths = [pth.strip() for pth in test_img_pths] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full path for each image in the testing set\n",
    "test_img_pths = [f'{ROOT_DIR}{pth}' for pth in test_img_pths]\n",
    "test_mask_pths = [pth.replace(\"JPEGImages\", \"Annotations\").replace(\"jpg\", \"png\") for pth in test_img_pths]\n",
    "# mask_paths = [f'{ROOT_DIR}/Annotations/{pth.replace(\"/JPEGImages/\", \"\").replace(\"jpg\", \"png\")}' for pth in img_pths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_img_pths, test_mask_pths))\n",
    "test_dataset = test_dataset.map(load_image_and_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations from the VIL-100 test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's input structure\n",
    "print(\"Model input:\", model.input)  # This tells you what kind of input the model expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of investigating the 100th image in the testing set\n",
    "count = 0\n",
    "\n",
    "# Correct way to iterate over a TensorFlow dataset\n",
    "for data in test_dataset:\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "    if count == 100:  \n",
    "        # If 'data' is a tuple, extract the relevant part\n",
    "        if isinstance(data, tuple):\n",
    "            image_data = data[0]  # Extract the input tensor\n",
    "        else:\n",
    "            image_data = data  # If there's no tuple, use the data directly\n",
    "        \n",
    "        # Now use the correct input for prediction\n",
    "        predictions = model.predict(image_data)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Draw the binary mask image\n",
    "\n",
    "# Remove the batch dimension to get the image data\n",
    "image_data = predictions[0]  # Extract the first (and only) image\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert values based on the threshold\n",
    "# If value >= threshold, round to 1.0, otherwise round to 0.0\n",
    "image_data = np.where(image_data >= threshold, 1.0, 0.0)\n",
    "\n",
    "# Since it has a single channel, we need to remove the last dimension\n",
    "image_data = image_data.squeeze()\n",
    "\n",
    "# Plot the image using a grayscale color map\n",
    "plt.imshow(image_data, cmap='gray')  # Use 'gray' colormap for grayscale images\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()  # Display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mask image's data type\n",
    "mask_image = image_data.astype(np.uint8)\n",
    "mask_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the batch information from the original matrix\n",
    "org_image = data[0]\n",
    "\n",
    "org_image = tf.squeeze(org_image, axis=0).numpy()\n",
    "org_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the original image\n",
    "\n",
    "plt.imshow(org_image)\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()\n",
    "plt.savefig('img/normal_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the image with the detected lanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the base image and mask are the same size\n",
    "assert org_image.shape[:2] == mask_image.shape, \"The base image and mask must have the same dimensions.\"\n",
    "\n",
    "# Create a red overlay to apply where the mask is white\n",
    "red_overlay = np.zeros_like(org_image)\n",
    "red_overlay[:, :, 2] = 255  # Set the red channel to maximum (255)\n",
    "\n",
    "# Apply the mask to the red overlay to keep only the red in the white regions of the mask\n",
    "red_masked = cv2.bitwise_and(red_overlay, red_overlay, mask=mask_image)\n",
    "\n",
    "# Combine the base image and the red-masked overlay\n",
    "result_image = cv2.addWeighted(org_image, 1.0, red_masked, 1.0, 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization from the external data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Prediction for external pictures\n",
    "base_image_path = r\"..\\curve_lane.jpg\"\n",
    "\n",
    "base_image = load_image(base_image_path)\n",
    "base_image = resize_image(base_image, (960, 528))\n",
    "\n",
    "predictions = model.predict(base_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the batch dimension to get the image data\n",
    "image_data = predictions[0]  # Extract the first (and only) image\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "# Convert values based on the threshold\n",
    "# If value >= threshold, round to 1.0, otherwise round to 0.0\n",
    "image_data = np.where(image_data >= threshold, 1.0, 0.0)\n",
    "\n",
    "# Since it has a single channel, we need to remove the last dimension\n",
    "image_data = image_data.squeeze()\n",
    "\n",
    "# Plot the image using a grayscale color map\n",
    "plt.imshow(image_data, cmap='gray')  # Use 'gray' colormap for grayscale images\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()  # Display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data type of the mask image\n",
    "mask_image = image_data.astype(np.uint8)\n",
    "mask_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the batch information\n",
    "base_image = tf.squeeze(base_image, axis=0).numpy()\n",
    "base_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the base image and mask are the same size\n",
    "assert base_image.shape[:2] == mask_image.shape, \"The base image and mask must have the same dimensions.\"\n",
    "\n",
    "# Create a red overlay to apply where the mask is white\n",
    "red_overlay = np.zeros_like(base_image)\n",
    "red_overlay[:, :, 2] = 255  # Set the red channel to maximum (255)\n",
    "\n",
    "# Apply the mask to the red overlay to keep only the red in the white regions of the mask\n",
    "red_masked = cv2.bitwise_and(red_overlay, red_overlay, mask=mask_image)\n",
    "\n",
    "# Combine the base image and the red-masked overlay\n",
    "result_image = cv2.addWeighted(base_image, 1.0, red_masked, 1.0, 0)\n",
    "\n",
    "# Original size: (1080, 1920, 3)\n",
    "result_image = cv2.resize(result_image, (1080, 1920), interpolation=cv2.INTER_AREA)\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')  # Hide axis ticks and labels\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
